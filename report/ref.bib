

@ARTICLE{structural97,
  author={Fernandez-Gaucherand, E. and Marcus, S.I.},
  journal={IEEE Transactions on Automatic Control}, 
  title={Risk-sensitive optimal control of hidden Markov models: structural results}, 
  year={1997},
  volume={42},
  number={10},
  pages={1418-1422},
  keywords={Optimal control;Hidden Markov models;Statistics;Probability;Equations;Automatic control;Stochastic processes;Delta modulation;Cost function;Additives},
  doi={10.1109/9.633830}}


@inproceedings{chen2020learning,
  title={Learning by cheating},
  author={Chen, Dian and Zhou, Brady and Koltun, Vladlen and Kr{\"a}henb{\"u}hl, Philipp},
  booktitle={Conference on Robot Learning},
  pages={66--75},
  year={2020},
  organization={PMLR}
}

@article{pinto2017asymmetric,
  title={Asymmetric actor critic for image-based robot learning},
  author={Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1710.06542},
  year={2017}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}
@book{1998exact_approx_alg_pomdp,
  title={Exact and approximate algorithms for partially observable Markov decision processes},
  author={Cassandra, Anthony Rocco},
  year={1998},
  publisher={Brown University}
}
@article{drl_partial_risk,
title = {Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints},
journal = {Reliability Engineering \& System Safety},
volume = {212},
pages = {107551},
year = {2021},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2021.107551},
url = {https://www.sciencedirect.com/science/article/pii/S095183202100106X},
author = {C.P. Andriotis and K.G. Papakonstantinou},
keywords = {Inspection and maintenance planning, System risk and reliability, Constrained stochastic optimization, Partially observable Markov decision processes, Deep reinforcement learning, Decentralized multi-agent control}
}
@inproceedings{2019riskpomdp_autonomous_drive,
  title={Online risk-bounded motion planning for autonomous vehicles in dynamic environments},
  author={Huang, Xin and Hong, Sungkweon and Hofmann, Andreas and Williams, Brian C},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  volume={29},
  pages={214--222},
  year={2019}
}
@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}
@article{cao2020DRL_derivative_hedging,
  title={Deep hedging of derivatives using reinforcement learning},
  author={Cao, Jay and Chen, Jacky and Hull, John and Poulos, Zissis},
  journal={The Journal of Financial Data Science},
  year={2020},
  publisher={Institutional Investor Journals Umbrella}
}

@article{jiang2017DRL_protofolio,
  title={A deep reinforcement learning framework for the financial portfolio management problem},
  author={Jiang, Zhengyao and Xu, Dixing and Liang, Jinjun},
  journal={arXiv preprint arXiv:1706.10059},
  year={2017}
}
@article{richman2021ai_actuarial,
  title={AI in actuarial science--a review of recent advances--part 1},
  author={Richman, Ronald},
  journal={Annals of Actuarial Science},
  volume={15},
  number={2},
  pages={207--229},
  year={2021},
  publisher={Cambridge University Press}
}
@article{di1999risk,
  title={Risk sensitive control of discrete time partially observed Markov processes with infinite horizon},
  author={Di Masi, GB and Sthttner, L},
  journal={Stochastics: An International Journal of Probability and Stochastic Processes},
  volume={67},
  number={3-4},
  pages={309--322},
  year={1999},
  publisher={Taylor \& Francis}
}
@article{whittle1990risk,
  title={A risk-sensitive maximum principle},
  author={Whittle, P},
  journal={Systems \& Control Letters},
  volume={15},
  number={3},
  pages={183--192},
  year={1990},
  publisher={Elsevier}
}

@article{2022DRL_stock,
  title={Deep reinforcement learning approach for trading automation in the stock market},
  author={Kabbani, Taylan and Duman, Ekrem},
  journal={IEEE Access},
  volume={10},
  pages={93564--93574},
  year={2022},
  publisher={IEEE}
}




@inproceedings{doshi2008reinforcement,
  title={Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs},
  author={Doshi, Finale and Pineau, Joelle and Roy, Nicholas},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={256--263},
  year={2008}
}


@inproceedings{choi2021risk,
  title={Risk-conditioned distributional soft actor-critic for risk-sensitive navigation},
  author={Choi, Jinyoung and Dance, Christopher and Kim, Jung-Eun and Hwang, Seulbin and Park, Kyung-sik},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={8337--8344},
  year={2021},
  organization={IEEE}
}




@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{agarwal2019RLtheorybook,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  volume={32},
  year={2019}
}

@book{jazwinski2007StochasticFilteringTheory,
  title={Stochastic processes and filtering theory},
  author={Jazwinski, Andrew H},
  year={2007},
  publisher={Courier Corporation}
}

@book{oksendal2013StochasticCalculus,
  title={Stochastic differential equations: an introduction with applications},
  author={Oksendal, Bernt},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{baxter1996finCalculus,
  title={Financial calculus: an introduction to derivative pricing},
  author={Baxter, Martin and Rennie, Andrew},
  year={1996},
  publisher={Cambridge university press}
}

@book{albanese2005advanced,
  title={Advanced derivatives pricing and risk management: theory, tools, and hands-on programming applications},
  author={Albanese, Claudio and Campolieti, Giuseppe},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{chen2017cognitive,
  title={A cognitive model of how people make decisions through interaction with visual displays},
  author={Chen, Xiuli and Starke, Sandra Dorothee and Baber, Chris and Howes, Andrew},
  booktitle={Proceedings of the 2017 CHI conference on human factors in computing systems},
  pages={1205--1216},
  year={2017}
}


@misc{2021autodrive-POMDP-risk,
  title={Risk-aware autonomous driving using POMDPs and responsibility-sensitive safety},
  author={Skoglund, Caroline},
  year={2021}
}

@article{der2009aleatory,
  title={Aleatory or epistemic? Does it matter?},
  author={Der Kiureghian, Armen and Ditlevsen, Ove},
  journal={Structural safety},
  volume={31},
  number={2},
  pages={105--112},
  year={2009},
  publisher={Elsevier}
}

@article{2021aleatoricepistemic,
  title={Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods},
  author={H{\"u}llermeier, Eyke and Waegeman, Willem},
  journal={Machine Learning},
  volume={110},
  pages={457--506},
  year={2021},
  publisher={Springer}
}



@article{zhang2021MARLintro,
  title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of reinforcement learning and control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@article{Shen_2014,
   title={Risk-Sensitive Reinforcement Learning},
   volume={26},
   ISSN={1530-888X},
   url={http://dx.doi.org/10.1162/NECO_a_00600},
   DOI={10.1162/neco_a_00600},
   number={7},
   journal={Neural Computation},
   publisher={MIT Press - Journals},
   author={Shen, Yun and Tobia, Michael J. and Sommer, Tobias and Obermayer, Klaus},
   year={2014},
   month=jul, pages={1298â€“1328} 
}



@misc{tirinzoni2022PAC-conversion,
      title={Optimistic PAC Reinforcement Learning: the Instance-Dependent View}, 
      author={Andrea Tirinzoni and Aymen Al-Marjani and Emilie Kaufmann},
      year={2022},
      eprint={2207.05852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      pages={2,7}
}
@misc{jin2018qlearning,
      title={Is Q-learning Provably Efficient?}, 
      author={Chi Jin and Zeyuan Allen-Zhu and Sebastien Bubeck and Michael I. Jordan},
      year={2018},
      eprint={1807.03765},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}


%%%%%%%%
@article{parodi2009computational,
  title={Computational intelligence techniques for general insurance},
  author={Parodi, Pietro},
  journal={Research dissertation for the Actuarial Profession},
  year={2009}
}

@inproceedings{vassilev2022risk,
  title={Risk assessment in transactions under threat as partially observable Markov decision process},
  author={Vassilev, Vassil and Donchev, Doncho and Tonchev, Demir},
  booktitle={Optimization in Artificial Intelligence and Data Sciences: ODS, First Hybrid Conference, Rome, Italy, September 14-17, 2021},
  pages={199--212},
  year={2022},
  organization={Springer}
}




@article{Prashanth2022Wasserstein,
  title={A Wasserstein distance approach for concentration of empirical risk estimates},
  author={Prashanth, LA and Bhat, Sanjay P},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={10830--10890},
  year={2022},
  publisher={JMLRORG}
}


@article{Massart1990Tight,
  title={The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality},
  author={Massart, Pascal},
  journal={The annals of Probability},
  pages={1269--1283},
  year={1990},
  publisher={JSTOR}
}

@article{Weissman2003Inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}


@article{portfolio,
  title={Portfolio selection with imperfect information: A hidden Markov model},
  author={{\c{C}}anako{\u{g}}lu, Ethem and {\"O}zekici, S{\"u}leyman},
  journal={Applied Stochastic Models in Business and Industry},
  volume={27},
  number={2},
  pages={95--114},
  year={2011},
  publisher={Wiley Online Library}
}


@article{shi2021stock,
  title={Stock trading rule discovery with double deep Q-network},
  author={Shi, Yong and Li, Wei and Zhu, Luyao and Guo, Kun and Cambria, Erik},
  journal={Applied Soft Computing},
  volume={107},
  pages={107320},
  year={2021},
  publisher={Elsevier}
}

@article{lauri2022RoboticPOMDPsurvey,
  title={Partially observable markov decision processes in robotics: A survey},
  author={Lauri, Mikko and Hsu, David and Pajarinen, Joni},
  journal={IEEE Transactions on Robotics},
  volume={39},
  number={1},
  pages={21--40},
  year={2022},
  publisher={IEEE}
}

@inproceedings{hou2016solving,
  title={Solving risk-sensitive POMDPs with and without cost observations},
  author={Hou, Ping and Yeoh, William and Varakantham, Pradeep},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  year={2016}
}

@book{kreyszig1991functional_analysis,
  title={Introductory functional analysis with applications},
  author={Kreyszig, Erwin},
  volume={17},
  year={1991},
  publisher={John Wiley \& Sons}
}

@article{rudin1987real_analysis,
  title={Real and Complex Analysis. By W. Rudin. Pp. 412. 84s. 1966.(McGraw-Hill, New York.)},
  author={Rankin, RA},
  journal={The Mathematical Gazette},
  volume={52},
  number={382},
  pages={412--412},
  year={1968},
  publisher={Cambridge University Press}
}


@article{osband2016lower,
  title={On lower bounds for regret in reinforcement learning},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1608.02732},
  year={2016}
}


@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@book{concentration_martingales,
  title={Concentration inequalities for sums and martingales},
  author={Bercu, Bernard and Delyon, Bernard and Rio, Emmanuel and others},
  year={2015},
  publisher={Springer}
}

@book{2003concentration_book_Lugosi,
  title={Concentration inequalities},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Bousquet, Olivier},
  booktitle={Summer school on machine learning},
  pages={208--240},
  year={2003},
  publisher={Springer}
}

@article{highdimprob_book,
  title={High-dimensional probability},
  author={Vershynin, Roman},
  journal={University of California, Irvine},
  year={2020}
}

@inproceedings{bai2020provable,
  title={Provable self-play algorithms for competitive reinforcement learning},
  author={Bai, Yu and Jin, Chi},
  booktitle={International conference on machine learning},
  pages={551--560},
  year={2020},
  organization={PMLR}
}


@InProceedings{Fei_risk_fa,
  title = 	 {Risk-Sensitive Reinforcement Learning with Function Approximation: A Debiasing Approach},
  author =       {Fei, Yingjie and Yang, Zhuoran and Wang, Zhaoran},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3198--3207},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/Fei21improve/Fei21improve.pdf},
  url = 	 {https://proceedings.mlr.press/v139/Fei21improve.html},
}
% general=======================================================================================================================================================================================================================================
   @book{Ross2013appliedprob,
      title={Applied probability models with optimization applications},
      author={Ross, Sheldon M},
      year={2013},
      publisher={Courier Corporation}
   }
   % online-to-batch conversion
   @article{Cesa-Bianchi2004,
   author = {Cesa-Bianchi, NicolÃ² and Conconi, Alex and Gentile, Claudio},
   year = {2004},
   month = {10},
   pages = {2050 - 2057},
   title = {On the Generalization Ability of On-Line Learning Algorithms},
   volume = {50},
   journal = {Information Theory, IEEE Transactions on},
   doi = {10.1109/TIT.2004.833339}
   }

%============================================================================================================================================================================================================================
% POMDP
% POMDP surveyï¼š1950-1980
@article{MonahanSurvey1982,
  title={State of the artâ€”a survey of partially observable Markov decision processes: theory, models, and algorithms},
  author={Monahan, George E},
  journal={Management science},
  volume={28},
  number={1},
  pages={1--16},
  year={1982},
  publisher={INFORMS}
}
   % HMM
   @inproceedings{vogel1996hmm,
   title={HMM-based word alignment in statistical translation},
   author={Vogel, Stephan and Ney, Hermann and Tillmann, Christoph},
   booktitle={COLING 1996 Volume 2: The 16th International Conference on Computational Linguistics},
   year={1996}
   }
% modern application of POMDP survey.
   @article{RoboticSurvey2022,
      abstract = {Noisy sensing, imperfect control, and environment changes are defining characteristics of many real-world robot tasks. The partially observable Markov decision process (POMDP) provides a principled mathematical framework for modeling and solving robot decision and control tasks under uncertainty. Over the last decade, it has seen many successful applications, spanning localization and navigation, search and tracking, autonomous driving, multi-robot systems, manipulation, and human-robot interaction. This survey aims to bridge the gap between the development of POMDP models and algorithms at one end and application to diverse robot decision tasks at the other. It analyzes the characteristics of these tasks and connects them with the mathematical and algorithmic properties of the POMDP framework for effective modeling and solution. For practitioners, the survey provides some of the key task characteristics in deciding when and how to apply POMDPs to robot tasks successfully. For POMDP algorithm designers, the survey provides new insights into the unique challenges of applying POMDPs to robot systems and points to promising new directions for further research.},
      author = {Mikko Lauri and David Hsu and Joni Pajarinen},
      doi = {10.1109/TRO.2022.3200138},
      month = {9},
      title = {Partially Observable Markov Decision Processes in Robotics: A Survey},
      url = {http://arxiv.org/abs/2209.10342 http://dx.doi.org/10.1109/TRO.2022.3200138},
      year = {2022},
      }

   @misc{kurniawati2021,
         title={Partially Observable Markov Decision Processes (POMDPs) and Robotics}, 
         author={Hanna Kurniawati},
         year={2021},
         eprint={2107.07599},
         archivePrefix={arXiv},
         primaryClass={cs.RO}
   }

% Early results of POMDP
   % [negative] computation intractablity 
      @article{Tsitsiklis1987,
      ISSN = {0364765X, 15265471},
      URL = {http://www.jstor.org/stable/3689975},
      author = {Christos H. Papadimitriou and John N. Tsitsiklis},
      journal = {Mathematics of Operations Research},
      number = {3},
      pages = {441--450},
      publisher = {INFORMS},
      title = {The Complexity of Markov Decision Processes},
      urldate = {2023-09-18},
      volume = {12},
      year = {1987}
      }

@article{krishnamurthy2016pac,
  title={PAC reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

   % [possitive]  optimization and control
      @article{Striebel1965,
         title={Sufficient statistics in the optimum control of stochastic systems},
         author={Striebel, Charlotte},
         journal={Journal of Mathematical Analysis and Applications},
         volume={12},
         number={3},
         pages={576--592},
         year={1965},
         chapter={2},
         publisher={Academic Press}
      }
      @article{Smallwood&Sondik1971,
         author = {Richard D Smallwood and Edward J Sondik},
         issn = {1071-1088},
         issue = {5},
         journal = {Source: Operations Research},
         title = {The Optimal Control of Partially Observable Markov Processes Over a Finite Horizon},
         volume = {21},
         year={1972},
         url = {https://www.jstor.org/stable/168926?seq=1&cid=pdf-},
      }
      @article{Bertsekas1976Chap4.1,
         title={Dynamic programing and stochastic control},
         author={Bertsekas, Dimitri P},
         journal={Mathematics in science and engineering},
         volume={125},
         year={1976},
         chapter={4}, 
         page={117},
         publisher={Academic Press}
      }
      @article{Sondik1978,
         author = {Sondik, Edward},
         year = {1978},
         month = {04},
         pages = {282-304},
         title = {The Optimal Control of Partially Observable Markov Process over the Infinite Horizon: Discounted Costs},
         volume = {26},
         journal = {Operations Research},
         doi = {10.1287/opre.26.2.282}
      }
      @article{Vlassis2011,
         abstract = {We show that the problem of finding an optimal stochastic 'blind' controller in a Markov decision process is an NP-hard problem. The corresponding decision problem is NP-hard, in PSPACE, and SQRT-SUM-hard, hence placing it in NP would imply breakthroughs in long-standing open problems in computer science. Our result establishes that the more general problem of stochastic controller optimization in POMDPs is also NP-hard. Nonetheless, we outline a special case that is convex and admits efficient global solutions.},
         author = {Nikos Vlassis and Michael L. Littman and David Barber},
         month = {7},
         title = {On the Computational Complexity of Stochastic Controller Optimization in POMDPs},
         url = {http://arxiv.org/abs/1107.3090},
         year = {2011},
      }

% methods:
   % change of measure
      % HMM: comprehensive textbook see chapter 2.3 for "change of measure"
      @book{Elliott2008,
      title={Hidden Markov models: estimation and control},
      author={Elliott, Robert J and Aggoun, Lakhdar and Moore, John B},
      volume={29},
      year={2008},
      publisher={Springer Science \& Business Media}, 
      chapter={2.3},
      }
   
   % origin of OOM
      @article{Jaeger1997,
         abstract = {The article describes a new formal approach to model discrete stochastic processes , called observable operator models (OOMs). It is shown how hidden Markov models (HMMs) can be properly generalized to OOMs. These OOMs afford both mathematical simplicity and algorithmic efficiency, where HMMs exhibit neither. The observable operator idea also leads to an abstract, information-theoretic representation of stationary stochastic processes. It is shown how any such process can be uniquely characterized by linear, observable operators, yielding an abstract OOM of the process. All in all, observable operators open a lucid, general, and computationally extremely powerful avenue to stochastic processes.},
         author = {Herbert Jaeger},
         title = {A short introduction to observable operator models of discrete stochastic processes A short introduction to observable operator models of stochastic processes *},
         url = {https://www.researchgate.net/publication/2294860},
         year = {1997},
      }
      @report{Jaeger1998,
         author = {Herbert Jaeger},
         title = {Discrete-Time, Discrete-Valued Observable Operator Models: A Tutorial},
         url = {https://www.researchgate.net/publication/2258267},
         year={1998},
      }
      @report{Jaeger2017,
         author = {Herbert Jaeger},
         issue = {6},
         journal = {Appeared in Neural Computation},
         pages = {1371-1398},
         title = {Observable operator models for discrete stochastic time series 1 (with erratum note added August 2017)},
         volume = {12},
         year = {2017},
      }
   % spectral methods/tensor analysis
      @article{Anandkumar2012,
         abstract = {This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.},
         author = {Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and Matus Telgarsky},
         month = {10},
         title = {Tensor decompositions for learning latent variable models},
         url = {http://arxiv.org/abs/1210.7559},
         year = {2012},
      }
      @article{Hsu2008,
         abstract = {Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.},
         author = {Daniel Hsu and Sham M. Kakade and Tong Zhang},
         month = {11},
         title = {A Spectral Algorithm for Learning Hidden Markov Models},
         url = {http://arxiv.org/abs/0811.4413},
         year = {2008},
      }
      @article{Azizzadenesheli2016,
         author       = {Kamyar Azizzadenesheli and
                           Alessandro Lazaric and
                           Animashree Anandkumar},
         title        = {Reinforcement Learning of POMDP's using Spectral Methods},
         journal      = {CoRR},
         volume       = {abs/1602.07764},
         year         = {2016},
         url          = {http://arxiv.org/abs/1602.07764},
         eprinttype    = {arXiv},
         eprint       = {1602.07764},
         timestamp    = {Mon, 13 Aug 2018 16:46:51 +0200},
         biburl       = {https://dblp.org/rec/journals/corr/Azizzadenesheli16.bib},
         bibsource    = {dblp computer science bibliography, https://dblp.org}
      }
% sample efficiency for sub classes of MDP with imperfect information
   % latent MDPs
   @article{Kwon2021,
  title={Rl for latent mdps: Regret guarantees and a lower bound},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24523--24534},
  year={2021}
   }

   % overcomplete HMM
      @article{Sharan2017,
         abstract = {We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.},
         author = {Vatsal Sharan and Sham Kakade and Percy Liang and Gregory Valiant},
         month = {11},
         title = {Learning Overcomplete HMMs},
         url = {http://arxiv.org/abs/1711.02309},
         year = {2017},
      }

   % \gamma-observable POMDPs
     

   % alpha-revealing POMDPs
      % undercomplete
      @article{Jin2020,
      title={Sample-efficient reinforcement learning of undercomplete pomdps},
      author={Jin, Chi and Kakade, Sham and Krishnamurthy, Akshay and Liu, Qinghua},
      journal={Advances in Neural Information Processing Systems},
      volume={33},
      pages={18530--18539},
      year={2020}
      }
   % both under and overcomplete

% \gamma-observable POMDPs

% planning in POMDP with theoretical gaurantee
   @article{mcallester2013,
         title={Approximate Planning for Factored POMDPs using Belief State Simplification}, 
         author={David A. McAllester and Satinder Singh},
         year={2013},
         eprint={1301.6719},
         archivePrefix={arXiv},
         primaryClass={cs.AI}
   }
   @article{kara2022,
         title={Near Optimality of Finite Memory Feedback Policies in Partially Observed Markov Decision Processes}, 
         author={Ali Devran Kara and Serdar Yuksel},
         year={2022},
         eprint={2010.07452},
         archivePrefix={arXiv},
         primaryClass={math.OC}
   }
   @article{Golowich2022-a,
         author = {Noah Golowich and Ankur Moitra and Dhruv Rohatgi},
         month = {1},
         title = {Planning in Observable POMDPs in Quasipolynomial Time},
         url = {http://arxiv.org/abs/2201.04735},
         journal={arXiv preprint arXiv:2201.04735},
         year = {2022},
       }

% learning in POMDP with theoretical gaurantee
@article{Golowich2022-b,
     author = {Noah Golowich and Ankur Moitra and Dhruv Rohatgi},
     month = {6},
     title = {Learning in Observable POMDPs, without Computationally Intractable Oracles},
     url = {http://arxiv.org/abs/2206.03446},
     journal={Advances in Neural Information Processing Systems},
     year = {2022},
     }
@article{Liu2022,
     author = {Qinghua Liu and Alan Chung and Csaba SzepesvÃ¡ri and Szepesva@ualberta Ca and Chi Jin and Po-Ling Loh and Maxim Raginsky},
     journal = {Proceedings of Machine Learning Research},
     pages = {1-46},
     title = {When Is Partially Observable Reinforcement Learning Not Scary?},
     volume = {178},
     year = {2022},
}
@misc{Liu2022Games,
  title={Sample-Efficient Reinforcement Learning of Partially Observable Markov Games}, 
  author={Qinghua Liu and Csaba SzepesvÃ¡ri and Chi Jin},
  year={2022},
  eprint={2206.01315},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@misc{Liu2022General,
  title={Optimistic MLE -- A Generic Model-based Algorithm for Partially Observable Sequential Decision Making}, 
  author={Qinghua Liu and Praneeth Netrapalli and Csaba SzepesvÃ¡ri and Chi Jin},
  year={2022},
  eprint={2209.14997},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


Near Optimality of Finite Memory Feedback Policies in POMDP, 
which defines the cost of belief MDP

% risk RL
% [basic formulation]====================================================================================================================================================================================================================================
   @article{Howard72,
      author = {Ronald A Howard and James E Matheson},
      issue = {7},
      journal = {Source: Management Science},
      pages = {356-369},
      title = {Risk-Sensitive Markov Decision Processes},
      volume = {18},
      url = {https://about.jstor.org/terms},
      year = {1972},
   }
    @incollection{heger1994consideration,
      title={Consideration of risk in reinforcement learning},
      author={Heger, Matthias},
      booktitle={Machine Learning Proceedings 1994},
      pages={105--111},
      year={1994},
      publisher={Elsevier}
    }
@article{Ruszczynski10,
      author = {Andrzej RuszczyÅ„ski},
      doi = {10.1007/s10107-010-0393-3},
      issn = {14364646},
      issue = {2},
      journal = {Mathematical Programming},
      keywords = {Dynamic risk measures,Markov risk measures,Min-max Markov models,Nonsmooth Newton's method,Policy iteration,Value iteration},
      pages = {235-261},
      publisher = {Springer Verlag},
      title = {Risk-averse dynamic programming for Markov decision processes},
      volume = {125},
      year = {2010},
   }

% risk RL surveys
   @article{SafeSurvey15,
      abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted fi-nite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
      author = {Javier GarcÃ­a and Fernando FernÃ¡ndez},
      journal = {Journal of Machine Learning Research},
      keywords = {reinforcement learning,risk sensitivity,safe exploration,teacher advice},
      pages = {1437-1480},
      title = {A Comprehensive Survey on Safe Reinforcement Learning},
      volume = {16},
      year = {2015},
   }
   @article{SafeSurvey22,
      title={A review of safe reinforcement learning: Methods, theory and applications},
      author={Gu, Shangding and Yang, Long and Du, Yali and Chen, Guang and Walter, Florian and Wang, Jun and Yang, Yaodong and Knoll, Alois},
      journal={arXiv preprint arXiv:2205.10330},
      year={2022}
    }

   @article{ConstrainSurvey21,
      abstract = {Reinforcement Learning (RL) algorithms have had tremendous success in simulated domains. These algorithms, however, often cannot be directly applied to physical systems, especially in cases where there are constraints to satisfy (e.g. to ensure safety or limit resource consumption). In standard RL, the agent is incentivized to explore any policy with the sole goal of maximizing reward; in the real world, however, ensuring satisfaction of certain constraints in the process is also necessary and essential. In this article, we overview existing approaches addressing constraints in model-free reinforcement learning. We model the problem of learning with constraints as a Constrained Markov Decision Process and consider two main types of constraints: cumulative and instantaneous. We summarize existing approaches and discuss their pros and cons. To evaluate policy performance under constraints, we introduce a set of standard benchmarks and metrics. We also summarize limitations of current methods and present open questions for future research.},
      author = {Yongshuai Liu and Avishai Halev and Xin Liu},
      keywords = {Constraints and SAT: General,Machine learning: General,Planning and scheduling: General},
      title = {Policy Learning with Constraints in Model-free Reinforcement Learning: A Survey},
      year = {2021},
   }
   % risk measures
   % risk measure introduction
      @article{DistortionIntro10,
      author = {Sereda, Ekaterina and Bronshtein, Efim and Rachev, Teodosii and Fabozzi, Frank and Sun, Edward and Stoyanov, Stoyan},
      year = {2010},
      month = {01},
      pages = {649-673},
      title = {Distortion Risk Measures in Portfolio Optimization},
      isbn = {978-0-387-77438-1},
      journal = {Handbook of Portfolio Construction: Contemporary Applications of Markowitz Techniques},
      doi = {10.1007/978-0-387-77439-8_25}
      }
   % General risk measure theory
      @article{Righi2018theory,
         title={A theory for combinations of risk measures},
         author={Righi, Marcelo Brutti},
         journal={arXiv preprint arXiv:1807.01977},
         year={2018}
      }
      @article{boda2006time,
         title={Time consistent dynamic risk measures},
         author={Boda, Kang and Filar, Jerzy A},
         journal={Mathematical Methods of Operations Research},
         volume={63},
         pages={169--186},
         year={2006},
         publisher={Springer}
      }
      @misc{hau2023dynamic,
         title={On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes}, 
         author={Jia Lin Hau and Erick Delage and Mohammad Ghavamzadeh and Marek Petrik},
         year={2023},
         eprint={2304.12477},
         archivePrefix={arXiv},
         primaryClass={math.OC}
    }
    @article{VonNeumann1947UtilityTheory,
      title={Theory of games and economic behavior Princeton},
      author={Von Neumann, John and Morgenstern, Oskar},
      journal={Princeton University Press},
      volume={1947},
      pages={1953},
      year={1944}
    }
   % Dynamic risk measure
      @article{WhyDyn12,
         author       = {Takayuki Osogami},
         title        = {Iterated risk measures for risk-sensitive Markov decision processes
                           with discounted cost},
         journal      = {CoRR},
         volume       = {abs/1202.3755},
         year         = {2012},
         url          = {http://arxiv.org/abs/1202.3755},
         eprinttype    = {arXiv},
         eprint       = {1202.3755},
         timestamp    = {Mon, 13 Aug 2018 16:48:17 +0200},
         biburl       = {https://dblp.org/rec/journals/corr/abs-1202-3755.bib},
         bibsource    = {dblp computer science bibliography, https://dblp.org}
      }

@article{shen2014risk,
  title={Risk-sensitive reinforcement learning},
  author={Shen, Yun and Tobia, Michael J and Sommer, Tobias and Obermayer, Klaus},
  journal={Neural computation},
  volume={26},
  number={7},
  pages={1298--1328},
  year={2014},
  publisher={MIT Press}
}

% works on specific risk measures
   @article{Fei20,
      title={Risk-sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret},
      author={Fei, Yingjie and Yang, Zhuoran and Chen, Yudong and Wang, Zhaoran and Xie, Qiaomin},
      journal={Advances in Neural Information Processing Systems},
      volume={33},
      pages={22384--22395},
      year={2020}
    }

   @article{Fei21improve,
    title={Exponential bellman equation and improved regret bounds for risk-sensitive reinforcement learning},
    author={Fei, Yingjie and Yang, Zhuoran and Chen, Yudong and Wang, Zhaoran},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    pages={20436--20446},
    year={2021}
    }

   @report{Fei21b,
      abstract = {We study function approximation for episodic reinforcement learning with entropic risk measure. We first propose an algorithm with linear function approximation. Compared to existing algorithms, which suffer from improper regularization and regression biases, this algorithm features debiasing transformations in backward induction and regression procedures. We further propose an algorithm with general function approximation, which is shown to perform implicit debiasing transformations. We prove that both algorithms achieve a sublinear regret and demonstrate a tradeoff between generality and efficiency. Our analysis provides a unified framework for function approximation in risk-sensitive reinforcement learning, which leads to the first sub-linear regret bounds in the setting.},
      author = {Yingjie Fei and Zhuoran Yang and Zhaoran Wang},
      title = {Risk-Sensitive Reinforcement Learning with Function Approximation: A Debiasing Approach},
      year = {2021},
   }
    @misc{ding2022nonstationary,
      title={Non-stationary Risk-sensitive Reinforcement Learning: Near-optimal Dynamic Regret, Adaptive Detection, and Separation Design}, 
      author={Yuhao Ding and Ming Jin and Javad Lavaei},
      year={2022},
      eprint={2211.10815},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
    }
   @article{CVaR15,
      abstract = {This thesis presents the Conditional Value-at-Risk concept 
      and combines an analysis that co
      vers its application as a risk measure and as a vector norm. For both areas of application the theory is revised in detail and examples are given to show how to apply the concept in practice. In the first part, CVaR as a risk measure is introduced and the analysis covers the mathematical definition of CVaR and different methods to calculate it. Then, CVaR optimization is analysed in the context of portfolio selection and how to apply CVaR optimization for hedging a portfolio consisting of options. The original contributions in this part are an alternative proof of Acerbi's Integral Formula in the continuous case and an explicit programme formulation for portfolio hedging. The second part first analyses the Scaled and Non-Scaled CVaR norm as new family of norms in $\mathbb\{R\}^n$ and compares this new norm family to the more widely known $L_p$ norms. Then, model (or signal) recovery problems are discussed and it is described how appropriate norms can be used to recover a signal with less observations than the dimension of the signal. The last chapter of this dissertation then shows how the Non-Scaled CVaR norm can be used in this model recovery context. The original contributions in this part are an alternative proof of the equivalence of two different characterizations of the Scaled CVaR norm, a new proposition that the Scaled CVaR norm is piecewise convex, and the entire \autoref\{chapter:Recovery_using_CVaR\}. Since the CVaR norm is a rather novel concept, its applications in a model recovery context have not been researched yet. Therefore, the final chapter of this thesis might lay the basis for further research in this area.},
      author = {Jakob Kisiala},
      month = {10},
      title = {Conditional Value-at-Risk: Theory and Applications},
      url = {http://arxiv.org/abs/1511.00140},
      year = {2015},
   }

   @article{IterCVaR14,
      abstract = {This paper considers a Markov decision process in Borel state and action spaces with the aggregated (or say iterated) coherent risk measure to be minimised. For this problem, we establish the Bellman optimality equation as well as the value and policy iteration algorithms, and show the existence of a deterministic stationary optimal policy. The cost function, while being allowed to be unbounded from below (in the sense that its negative part needs be bounded by some nonnegative real-valued possibly unbounded weight function), can be arbitrarily unbounded from above and possibly infinitely valued.},
      author = {Shanyun Chu and Yi Zhang},
      doi = {10.1080/00207179.2014.909947},
      issn = {13665820},
      issue = {11},
      journal = {International Journal of Control},
      keywords = {Iterated coherent risk measure,Markov decision process,Optimality equation},
      pages = {2286-2293},
      publisher = {Taylor and Francis Ltd.},
      title = {Markov decision processes with iterated coherent risk measures},
      volume = {87},
      year = {2014},
   }

   @inproceedings{IterCVaR22,
  title={Provably efficient risk-sensitive reinforcement learning: Iterated cvar and worst path},
  author={Du, Yihan and Wang, Siwei and Huang, Longbo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
    }

   @article{coache2023reinforcement,
   title={Reinforcement learning with dynamic convex risk measures},
   author={Coache, Anthony and Jaimungal, Sebastian},
   journal={Mathematical Finance},
   year={2023},
   publisher={Wiley Online Library}
   }
   @article{coache2022conditionally,
  title={Conditionally elicitable dynamic risk measures for deep reinforcement learning},
  author={Coache, Anthony and Jaimungal, Sebastian and Cartea, {\'A}lvaro},
  journal={arXiv preprint arXiv:2206.14666},
  year={2022}
}
@article{marzban2021deep,
  title={Deep reinforcement learning for equal risk pricing and hedging under dynamic expectile risk measures},
  author={Marzban, Saeed and Delage, Erick and Li, Jonathan Yumeng},
  journal={arXiv preprint arXiv:2109.04001},
  year={2021}
}
% works on subclasses of risk measures
   @article{Lipschitz23,
    title={Regret bounds for risk-sensitive reinforcement learning with lipschitz dynamic risk measures},
    author={Liang, Hao and Luo, Zhi-quan},
    journal={arXiv preprint arXiv:2306.02399},
    year={2023}
    }

   @report{Coherent23,
      abstract = {We study the risk-aware reinforcement learning (RL) problem in the episodic finite-horizon Markov decision process with unknown transition and reward functions. In contrast to the risk-neutral RL problem, we consider minimizing the risk of having low rewards, which arise due to the intrinsic randomness of the MDPs and imperfect knowledge of the model. Our work provides a unified framework to analyze the regret of risk-aware RL policy with coherent risk measures in conjunction with non-linear function approximation, which gives the first sub-linear regret bounds in the setting. Finally, we validate our theoretical results via empirical experiments on synthetic and real-world data.},
      author = {Thanh Lam and Arun Verma and Bryan Kian and Hsiang Low and Patrick Jaillet},
      title = {risk-aware reinforcement learning with coherent risk measures and non-linear function approximation},
      year={2023}
   }

% non-stationary risk-sensitive RL
   @report{NonStat23,
      abstract = {We study risk-sensitive reinforcement learning (RL) based on an entropic risk measure in episodic non-stationary Markov decision processes (MDPs). Both the reward functions and the state transition kernels are unknown and allowed to vary arbitrarily over time with a budget on their cumulative variations. When this variation budget is known a prior, we propose two restart-based algorithms, namely Restart-RSMB and Restart-RSQ, and establish their dynamic regrets. Based on these results , we further present a meta-algorithm that does not require any prior knowledge of the variation budget and can adaptively detect the non-stationarity on the exponential value functions. A dynamic regret lower bound is then established for non-stationary risk-sensitive RL to certify the near-optimality of the proposed algorithms. Our results also show that the risk control and the handling of the non-stationarity can be separately designed in the algorithm if the variation budget is known a prior, while the non-stationary detection mechanism in the adaptive algorithm depends on the risk parameter. This work offers the first non-asymptotic theoretical analyses for the non-stationary risk-sensitive RL in the literature.},
      author = {Yuhao Ding and Ming Jin and Javad Lavaei},
      keywords = {Machine Learning (ML): ML: Reinforcement Learning Algorithms,Machine Learning (ML): ML: Reinforcement Learning Theory},
      title = {Non-stationary Risk-Sensitive Reinforcement Learning: Near-Optimal Dynamic Regret, Adaptive Detection, and Separation Design},
      url = {www.aaai.org},
      year = {2023},
   }

%================================================================================================================================================================================================================================================

% [risk-pomdp]=======================================================================================================================================================================================================================================
% 81-05 kill observation
   @article{WATER&Willems81,
      author = {Henk Van De WATER and Jan C Willems},
      issue = {5},
      journal = {IEEE TRANSACTIONS ON AUTOMATIC CONTROL},
      title = {The Certainty Equivalence Property in Stochastic Control Theory},
      year = {1981},
   }
   @article{Whittle91,
  author={Whittle, P.},
  journal={IEEE Transactions on Automatic Control}, 
  title={A risk-sensitive maximum principle: the case of imperfect state observation}, 
  year={1991},
  volume={36},
  number={7},
  pages={793-801},
  doi={10.1109/9.85059}}

   @article{Baras&Elliott94,
      title = {Risk-Sensitive Control and Dynamic Games for Partially Observed Discrete-Time Nonlinear Systems},
      author = {Matthew R. James and John S. Baras and Robert J. Elliott},
      doi = {10.1109/9.286253},
      issn = {15582523},
      issue = {4},
      journal = {IEEE Transactions on Automatic Control},
      pages = {780-792},
      
      volume = {39},
      year = {1994},
   }

   @article{Baras97,
      author = {J S Baras and M R James},
      issue = {3},
      journal = {J. Math. Systems, Estimation \& Control},
      keywords = {49K35,93C41,93E20, Robust control,fi-nite state machines,hidden Markov models AMS subject classifications (1991): 93B36,output feedback dynamic games,output feedback risk-sensitive stochastic optimal control},
      pages = {371-374},
      title = {Robust and Risk-Sensitive Output Feedback Control for Finite State Machines and Hidden Markov Models},
      volume = {7},
      url = {http://spigot.anu.edu.au/people/mat/home.html},
      year = {1997},
   }

   @article{Stettner98,
      author = {G. B. Di Masi and L. Stettner},
      doi = {10.1080/17442509908834216},
      issn = {01912216},
      journal = {Proceedings of the IEEE Conference on Decision and Control},
      pages = {3467-3472},
      publisher = {IEEE},
      title = {Risk sensitive control of discrete time partially observed Markov processes with infinite horizon},
      volume = {3},
      year = {1998},
   }

   @article{Elliott96,
   title = {Risk-Sensitive Maximum Likelihood Sequence Estimation},
   journal = {IFAC Proceedings Volumes},
   volume = {29},
   number = {1},
   pages = {4616-4621},
   year = {1996},
   note = {13th World Congress of IFAC, 1996, San Francisco USA, 30 June - 5 July},
   issn = {1474-6670},
   doi = {https://doi.org/10.1016/S1474-6670(17)58410-4},
   url = {https://www.sciencedirect.com/science/article/pii/S1474667017584104},
   author = {Robert J. Elliott and John B. Moore and Subhrakanti Dey},
   keywords = {Viterbi algorithm, risk-sensitive estimation, Maximum Likelihood sequence estimation, hidden Markov model},
   abstract = {In this paper, WE CONSIDER RISK-SENSITIVE MAXIMUM LIKELIHOOD SEQUENCE
    ESTIMATION FOR HIDDEN MARKOV MODELS WITH FINITE-DISCRETE STATES.
    An algorithm is proposed which is essentially 
    A RISK-SENSITIVE VARIATION OF THE VITERBI ALGORITHM. 
      SIMULATION STUDIES SHOW THAT THE RISK-SENSITIVE ALGORITHM IS MORE ROBUST TO UNCERTAINTIES IN THE TRANSITION
     PROBABILITY MATRIX OF THE MARKOV CHAIN. 
     Similar estimation results are also obtained for cont
     inuous-range state models.}
   }

   @article{Cavazos2005,
  title={Successive approximations in partially observable controlled Markov chains with risk-sensitive average criterion},
  author={Cavazos-Cadena, Rolando and Hern{\'a}ndez-Hern{\'a}ndez, Daniel},
  journal={Stochastics: An International Journal of Probability and Stochastics Processes},
  volume={77},
  number={6},
  pages={537--568},
  year={2005},
  publisher={Taylor \& Francis}
}

% 11-15:kill risk (German ideas)
@book{MDPFinbook11,
  title={Markov decision processes with applications to finance},
  author={B{\"a}uerle, Nicole and Rieder, Ulrich},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{baauerle_Rieder14,
  title={More risk-sensitive Markov decision processes},
  author={B{\"a}uerle, Nicole and Rieder, Ulrich},
  journal={Mathematics of Operations Research},
  volume={39},
  number={1},
  pages={105--120},
  year={2014},
  publisher={INFORMS}
}

@article{baauerle2017partially,
  title={Partially observable risk-sensitive Markov decision processes},
  author={B{\"a}auerle, Nicole and Rieder, Ulrich},
  journal={Mathematics of Operations Research},
  volume={42},
  number={4},
  pages={1180--1196},
  year={2017},
  publisher={INFORMS}
} 

% hindsight observability=========================
@misc{Lee2023hindsight,
      title={Learning in POMDPs is Sample-Efficient with Hindsight Observability}, 
      author={Jonathan N. Lee and Alekh Agarwal and Christoph Dann and Tong Zhang},
      year={2023},
      eprint={2301.13857},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{sinclair2023hindsight,
  title={Hindsight learning for mdps with exogenous inputs},
  author={Sinclair, Sean R and Frujeri, Felipe Vieira and Cheng, Ching-An and Marshall, Luke and Barbalho, Hugo De Oliveira and Li, Jingling and Neville, Jennifer and Menache, Ishai and Swaminathan, Adith},
  booktitle={International Conference on Machine Learning},
  pages={31877--31914},
  year={2023},
  organization={PMLR}
}

@article{shi2023theoretical,
  title={Theoretical Hardness and Tractability of POMDPs in RL with Partial Hindsight State Information},
  author={Shi, Ming and Liang, Yingbin and Shroff, Ness},
  journal={arXiv preprint arXiv:2306.08762},
  year={2023}
}

@article{guo2023sample,
  title={Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight},
  author={Guo, Jiacheng and Chen, Minshuo and Wang, Huan and Xiong, Caiming and Wang, Mengdi and Bai, Yu},
  journal={arXiv preprint arXiv:2307.02884},
  year={2023}
}


% PSR
@inproceedings{PSR,
 author = {Littman, Michael and Sutton, Richard S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {Predictive Representations of State},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1e4d36177d71bbb3558e43af9577d70e-Paper.pdf},
 volume = {14},
 year = {2001}
}

@misc{2009PSR-seminal,
      title={Closing the Learning-Planning Loop with Predictive State Representations}, 
      author={Byron Boots and Sajid M. Siddiqi and Geoffrey J. Gordon},
      year={2009},
      eprint={0912.2385},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{uehara2022PSR,
      title={Provably Efficient Reinforcement Learning in Partially Observable Dynamical Systems}, 
      author={Masatoshi Uehara and Ayush Sekhari and Jason D. Lee and Nathan Kallus and Wen Sun},
      year={2022},
      eprint={2206.12020},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{Sun2022PSR,
      title={PAC Reinforcement Learning for Predictive State Representations}, 
      author={Wenhao Zhan and Masatoshi Uehara and Wen Sun and Jason D. Lee},
      year={2022},
      eprint={2207.05738},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Yang2022POMDPfa,
      title={Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency}, 
      author={Qi Cai and Zhuoran Yang and Zhaoran Wang},
      year={2022},
      eprint={2204.09787},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}




@article{ROPONEN2020306,
title = {Adversarial risk analysis under partial information},
journal = {European Journal of Operational Research},
volume = {287},
number = {1},
pages = {306-316},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720303908},
author = {Juho Roponen and David {RÃ­os Insua} and Ahti Salo},
keywords = {Risk analysis, Decision analysis, Game theory, Stochastic dominance, Combat modeling},
}

@article{PBVI,
   title={Anytime Point-Based Approximations for Large POMDPs},
   volume={27},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.2078},
   DOI={10.1613/jair.2078},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Pineau, J. and Gordon, G. and Thrun, S.},
   year={2006},
   month=nov, pages={335â€“380} }


%empirical risk_POMDP
    @article{qiu2021rmix,
      title={RMIX: Learning risk-sensitive policies for cooperative reinforcement learning agents},
      author={Qiu, Wei and Wang, Xinrun and Yu, Runsheng and Wang, Rundong and He, Xu and An, Bo and Obraztsova, Svetlana and Rabinovich, Zinovi},
      journal={Advances in Neural Information Processing Systems},
      volume={34},
      pages={23049--23062},
      year={2021}
    }
@article{shen2023riskq,
  title={RiskQ: risk-sensitive multi-agent reinforcement learning value factorization},
  author={Shen, Siqi and Ma, Chennan and Li, Chao and Liu, Weiquan and Fu, Yongquan and Mei, Songzhu and Liu, Xinwang and Wang, Cheng},
  journal={arXiv preprint arXiv:2311.01753},
  year={2023}
}
